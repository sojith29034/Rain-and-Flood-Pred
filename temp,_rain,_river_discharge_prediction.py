# -*- coding: utf-8 -*-
"""Temp, Rain, River Discharge Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ULKd4RO5F01kcnifM9TRrwikDwcvC9Fm
"""

!pip install openmeteo-requests requests-cache retry-requests numpy pandas keras-tuner

import openmeteo_requests
import requests_cache
import pandas as pd
from retry_requests import retry

# Setup the Open-Meteo API client with cache and retry on error
cache_session = requests_cache.CachedSession('.cache', expire_after=3600)
retry_session = retry(cache_session, retries=5, backoff_factor=0.2)
openmeteo = openmeteo_requests.Client(session=retry_session)

# Fetch weather stats data
weather_url = "https://archive-api.open-meteo.com/v1/archive"
weather_params = {
    "latitude": 59.91,
    "longitude": 10.75,
    "start_date": "2020-01-01",
    "end_date": "2023-12-31",
    "daily": ["temperature_2m_max", "temperature_2m_min", "precipitation_sum", "rain_sum", "precipitation_hours", "wind_speed_10m_max", "wind_gusts_10m_max"],
    "timezone": "GMT"
}
weather_responses = openmeteo.weather_api(weather_url, params=weather_params)
weather_response = weather_responses[0]

daily = weather_response.Daily()
daily_data_weather = {
     "date": pd.date_range(
        start=pd.to_datetime(daily.Time(), unit="s", utc=True).tz_localize(None),  # Remove timezone here
        end=pd.to_datetime(daily.TimeEnd(), unit="s", utc=True).tz_localize(None),  # Remove timezone here
        freq=pd.Timedelta(seconds=daily.Interval()),
        inclusive="left"
    ),
    "temperature_2m_max": daily.Variables(0).ValuesAsNumpy(),
    "temperature_2m_min": daily.Variables(1).ValuesAsNumpy(),
    "rain_sum": daily.Variables(2).ValuesAsNumpy(),
    "precipitation_hours": daily.Variables(3).ValuesAsNumpy(),
    "wind_speed_10m_max": daily.Variables(4).ValuesAsNumpy(),
    "wind_gusts_10m_max": daily.Variables(5).ValuesAsNumpy()
}

weather_dataframe = pd.DataFrame(data=daily_data_weather)

# Fetch river discharge data
discharge_url = "https://flood-api.open-meteo.com/v1/flood"
discharge_params = {
    "latitude": 59.91,
    "longitude": 10.75,
    "daily": "river_discharge",
    "start_date": "2020-01-01",
    "end_date": "2023-12-31"
}
discharge_responses = openmeteo.weather_api(discharge_url, params=discharge_params)
discharge_response = discharge_responses[0]

daily_discharge = discharge_response.Daily()
daily_data_discharge = {
     "date": pd.date_range(
        start=pd.to_datetime(daily.Time(), unit="s", utc=True).tz_localize(None),
        end=pd.to_datetime(daily.TimeEnd(), unit="s", utc=True).tz_localize(None),
        freq=pd.Timedelta(seconds=daily.Interval()),
        inclusive="left"
    ),
    "river_discharge": daily_discharge.Variables(0).ValuesAsNumpy()
}

discharge_dataframe = pd.DataFrame(data=daily_data_discharge)

combined_df = pd.merge(weather_dataframe, discharge_dataframe, on="date", how="outer")

combined_df

"""# Normal LSTM"""

# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import MinMaxScaler
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import LSTM, Dense, Dropout
# from sklearn.metrics import mean_absolute_percentage_error

# # Assuming combined_df is already created with the required columns and date column
# combined_df['date'] = pd.to_datetime(combined_df['date']).dt.tz_localize(None)
# combined_df.set_index('date', inplace=True)

# # Select the features for training
# features = ['temperature_2m_max', 'temperature_2m_min', 'rain_sum',
#             'precipitation_hours', 'wind_speed_10m_max',
#             'wind_gusts_10m_max', 'river_discharge']

# data = combined_df[features].values

# # Scaling the data (LSTMs perform better with scaled data)
# scaler = MinMaxScaler(feature_range=(0, 1))
# scaled_data = scaler.fit_transform(data)

# # Create sequences and corresponding labels (next step prediction)
# def create_sequences(data, time_steps=60):
#     X, y = [], []
#     for i in range(len(data) - time_steps):
#         X.append(data[i:i + time_steps])
#         y.append(data[i + time_steps])  # predicting the next time step
#     return np.array(X), np.array(y)

# time_steps = 60  # The number of previous time steps to use for prediction
# X, y = create_sequences(scaled_data, time_steps)

# # Split the data into training and testing sets
# train_size = int(0.8 * len(X))
# X_train, X_test = X[:train_size], X[train_size:]
# y_train, y_test = y[:train_size], y[train_size:]

# # Build LSTM model
# model = Sequential()
# model.add(LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
# model.add(Dropout(0.2))  # To prevent overfitting
# model.add(LSTM(units=100))
# model.add(Dropout(0.2))
# model.add(Dense(units=X_train.shape[2]))  # Output layer (multivariate)

# # Compile the model
# model.compile(optimizer='adam', loss='mean_squared_error')

# # Train the model
# model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))

# # Predict
# y_pred = model.predict(X_test)

# # Inverse transform to get the actual values
# y_test_rescaled = scaler.inverse_transform(y_test)
# y_pred_rescaled = scaler.inverse_transform(y_pred)

# # Calculate MAPE
# mape = mean_absolute_percentage_error(y_test_rescaled, y_pred_rescaled)
# print(f"Test MAPE: {mape}")

# # Visualize results
# import matplotlib.pyplot as plt

# plt.plot(y_test_rescaled[:, -1], label='True River Discharge')
# plt.plot(y_pred_rescaled[:, -1], label='Predicted River Discharge')
# plt.title('River Discharge Prediction with LSTM')
# plt.xlabel('Time steps')
# plt.ylabel('River Discharge')
# plt.legend()
# plt.show()

"""# HyperParamter Tuning"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from keras_tuner import RandomSearch
from sklearn.metrics import mean_absolute_percentage_error

combined_df['date'] = pd.to_datetime(combined_df['date']).dt.tz_localize(None)
combined_df.set_index('date', inplace=True)

# Select the features for training
features = ['temperature_2m_max', 'temperature_2m_min', 'rain_sum',
            'precipitation_hours', 'wind_speed_10m_max',
            'wind_gusts_10m_max', 'river_discharge']

data = combined_df[features].values

# Scaling the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Create sequences and corresponding labels
def create_sequences(data, time_steps=60):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:i + time_steps])
        y.append(data[i + time_steps])
    return np.array(X), np.array(y)

time_steps = 60  # The number of previous time steps to use for prediction
X, y = create_sequences(scaled_data, time_steps)

# Split the data into training and testing sets
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Define the model building function for hyperparameter tuning
def build_model(hp):
    model = Sequential()

    # Tune the number of units in the first LSTM layer
    model.add(LSTM(units=hp.Int('units_1', min_value=50, max_value=200, step=50),
                   return_sequences=True,
                   input_shape=(X_train.shape[1], X_train.shape[2])))

    # Tune the dropout rate for the first layer
    model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))

    # Tune the number of units in the second LSTM layer
    model.add(LSTM(units=hp.Int('units_2', min_value=50, max_value=200, step=50)))

    # Tune the dropout rate for the second layer
    model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))

    # Output layer (multivariate, so use Dense with output size equal to features)
    model.add(Dense(units=X_train.shape[2]))

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),
                  loss='mean_squared_error')

    return model

# Initialize the KerasTuner RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=5,  # Number of different hyperparameter combinations to try
    executions_per_trial=1,  # Number of models to build and evaluate for each trial
    directory='tuning_results',
    project_name='lstm_hyperparameter_tuning'
)

# Run hyperparameter search
tuner.search(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))

# Get the best model
best_model = tuner.get_best_models(num_models=1)[0]

# Train the best model
best_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))

# Predict using the best model
y_pred = best_model.predict(X_test)

# Inverse transform to get the actual values
y_test_rescaled = scaler.inverse_transform(y_test)
y_pred_rescaled = scaler.inverse_transform(y_pred)

# Calculate MAPE
mape = mean_absolute_percentage_error(y_test_rescaled, y_pred_rescaled)
print(f"Test MAPE: {mape}")

import matplotlib.pyplot as plt

plt.plot(y_test_rescaled[:, -1], label='True River Discharge')
plt.plot(y_pred_rescaled[:, -1], label='Predicted River Discharge')
plt.title('River Discharge Prediction with LSTM (Tuned)')
plt.xlabel('Time steps')
plt.ylabel('River Discharge')
plt.legend()
plt.show()

model.save('my_model2.keras')

"""# OpenAI API"""

!pip install openai ipywidgets
!pip install openai==0.28

import openai
import ipywidgets as widgets
from IPython.display import display

# Set your OpenAI API key
openai.api_key = YOUR_API_KEY_HERE

def get_reply(prompt):
    prompt = f"{prompt}, {combined_df}"

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=150
    )

    return response['choices'][0]['message']['content'].strip()

prompt_input = widgets.Text(
    value='Give analysis of this data',
    placeholder='Enter a prompt',
    description='data',
    disabled=False
)

output = widgets.Output()

def on_button_click(b):
    with output:
        output.clear_output()
        prompt = prompt_input.value
        reply = get_reply(prompt)
        print(f"Reply to {prompt}:\n")
        print(reply)

button = widgets.Button(
    description='Generate',
    disabled=False,
    button_style='success',
    tooltip='Click to generate',
)

button.on_click(on_button_click)

display(prompt_input, button, output)